# ðŸ“ˆ Time Series Foundation Model Benchmarking

This repository provides a flexible benchmarking pipeline for time series (foundation) models.  
It allows to systematically run experiments across different **datasets, models, preprocessing methods, and training strategies**, and stores results in a structured format for later analysis.

## Features
- Benchmarking of time series (foundation) models on forecasting tasks
- Modular design to integrate custom models and datasets.  
- Configurable preprocessing (normalization, missing value handling).  
- Automatic tracking, logging, and structured saving of results (`y_true`, `y_pred`, configs).  
- Load and explore results programmatically for analysis and visualization.
- Currently, experiments are based on the **last window** of each time series, from which context and forecast horizon are derived.  
- At the moment only **zero-shot forecasting** is supported;  
  **few-shot** and **full-shot** capabilities will be added in the future.    


## Repository Structure

```text
.
â”œâ”€â”€ main.py                           # Entry point for benchmarking
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py                      # Utility functions (job generation, runner, etc.)
â”‚   â”œâ”€â”€ constants.py                  # Paths and global constants
â”‚   â”œâ”€â”€ logging.py                    # Logging setup
â”‚   â”œâ”€â”€ models/                       # Time series foundation models
â”‚   â”œâ”€â”€ datasets/                     # Data handling
â”‚   â””â”€â”€ evaluation/                   # Evaluation metrics
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ experiment.yaml               # Experiment configuration
â”‚   â”œâ”€â”€ dataset_config_map.yaml       # Dataset definitions
â”‚   â””â”€â”€ model_config_map.yaml         # Model definitions
â”œâ”€â”€ results/                          # Results folder
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”œâ”€â”€ data/                             # Raw input data
â”‚   â””â”€â”€ univariate_raw/
â”œâ”€â”€ logs/                             # Training / experiment logs
â”œâ”€â”€ pyproject.toml                    # uv setup
â”œâ”€â”€ uv.lock
â”œâ”€â”€ README.md
```

## Configuration

The benchmarking pipeline is fully driven by YAML config files.

### 1. Experiment Config (`experiment.yaml`)

This is the main YAML defining the *grid of benchmarking experiments* that will be run.

Example config:
```yaml
experiment:
  datasets:
    - example_all

  context_and_horizon:
    - [512, 96]
    - [1024, 96]

  random_seed:
    - 2
    - 3

  data_preprocessing:
    normalization:
      - "z-score"
    missing_value_handling:
      - "dropna"

  models:
    - example_model_epoch_100_batch_64
    - example_model_epoch_500_batch_64

  fit_strategy:
    - fit_strategy_name: "zero-shot"
```
Each experiment is expanded into all possible combinations of these parameters.

In this example:
- 1 dataset_collection (assuming **10** datasets in `examples_all`)
- 2 context/horizon settings
- 2 random seed settings
- 1 normalization setting
- 1 missing value handling setting
- 2 model setting
- 1 fit strategy

This results in **10 Ã— 2 Ã— 2 Ã— 1 Ã— 1 Ã— 2 Ã— 1 = 80 benchmark jobs** being executed.

For each experiment, a separate folder inside the `results/` directory will be created.
Each folder contains the predictions (`y_true.npy`, `y_pred.npy`) and the corresponding `config.json`.

To keep the `experiment.yaml` clean, datasets and models are only referenced by name. The actual dataset and model definitions are in `dataset_config_map.yaml` and `model_config_map.yaml`. During job generation, the names in `experiment.yaml` are resolved against these config files. Accordingly each dataset or model config in the `experiment.yaml` must exist in the `dataset_config_map.yaml` or `model_config_map.yaml` respectively.

### 2. Dataset Config (`dataset_config_map.yaml`)
Defines dataset collections that can applied in the `experiment.yaml`. Collections can be defined either by explicit file lists or by file patterns. Once a dataset collection is defined, its name can be referenced in the `experiment.yaml`
```yaml
datasets:
  - name: example_1_10
    class: UnivariateDataset
    data_path: univariate_raw
    files:
      - example_1.csv
      - example_10.csv

  - name: example_all
    class: UnivariateDataset
    data_path: univariate_raw
    file_pattern: "example*.csv"

  - ... 
---
```
### 3. Model Config (`model_config_map.yaml`)
Defines available models and their parameters. Each model may define its own set of parameters and model-specific options. Once a model is defined, its name can be referenced in the `experiment.yaml`
```yaml
models
  - name: example_model_epoch_100_batch_64
    class: "ExampleModel"
    params:
      epochs: 100
      batch_size: 64

  - ...

---
```

## Track experiment runs through `benchmark_summary.csv`

All experiment runs are tracked in a global benchmark_summary file: **`benchmark_summary.csv`**.  
This file is automatically created in the project root on the first run.  

### What does it contain?
- One row per executed benchmark job  
- Metadata from the benchmark job config
- Path to the corresponding result folder (`result_folder`)  
- A timestamp of when the run finished  

### Avoiding duplicate runs
Before starting a new job, the pipeline checks whether the job has **already been completed**

A job is considered *done* if all relevant configuration columns in the job dictionary match **exactly** with an entry in `benchmark_summary.csv`.   

If a match is found, the job is **skipped** to avoid duplicate work.

### Appending new results
When a job finishes, the pipeline:
1. Stores the results in the `results/` folder under a corresponding path.  
2. Appends a new row with the jobâ€™s metadata to the `benchmark_summary.csv` file.  

Over time, `benchmark_summary.csv` serves as a **central experiment log**, making it easy to filter, load and reproduce results.  

## Results

Each experiment run automaitcally stores:
- y_true.npy â€“ ground truth values  
- y_pred.npy â€“ predicted values  
- config.json â€“ full job configuration  

in a folder following a structured save path:
```text
results/
â””â”€â”€ <model_name>/
    â””â”€â”€ <configs_short_name>/
        â””â”€â”€ <timestep_datasetname>/
            â”œâ”€â”€ y_true.npy
            â”œâ”€â”€ y_pred.npy
            â””â”€â”€ config.json

```

## Adding a new model class
To integrate a new time series (foundation) model into the benchmarking framework, follow these steps:

**1. Create a new file in src/models/**
- The filename must be the snake_case version of the class name.
- Example: `ttm_sktime.py` â†’ `class TtmSktime`.

**2. Inherit from BaseModel**
- Every model must implement the following abstract methods:
  - `_init_model(self)` â€“ initialize the underlying forecasting model.
  - `prepare_zero_shot(self, series_df: pd.DataFrame)` â€“ prepare the model for zero-shot forecasting.
  - `predict(self)` â€“ return predictions for the specified horizon.
  - few-shot and full-shot are currently not implemented, but it is planned to integrate them later

**3. Define the modelâ€™s constructor `(__init__)`**
- Accept all necessary parameters via **params.
- Store them as attributes and call `_init_model()` inside `__init__`.

The easiest way to implement a new model is to copy the structure of an existing one (e.g. `TtmSktime`) and adapt it to the new library/model.

## Adding a new dataset class
To integrate a new dataset class into the benchmarking framework, follow these steps:

**1. Create a new file in src/models/**
- The filename must be the snake_case version of the class name.
- Example: `univariate_dataset.py` â†’ `class UnivariateDataset`.

**2. Inherit from BaseDataset**
Every dataset class must implement:
- `load_preprocessed_data(self, ..) -> pd.DataFrame`
- This method is responsible for loading raw data, applying missing value handling, normalization, and returning DataFrames ready for benchmarking.
- Column naming: after load_preprocessed_data() the dataframe must provide two columns named timestamp (time information) and value (observations).

## Load and explore benchmarking results
Use the `load_benchmarking_results` function from `src/utils.py` to load benchmarking results.

You can filter by dataset, model, forecast horizon, normalization, etc.
An example workflow is shown in `notebooks/01_explore_benchmarking_results.ipynb`, where results are loaded, filtered, metrics are computed, and models are compared visually.

## Start & Installation  

**Requirements:**  
- Python >=3.11
- [uv](https://docs.astral.sh/uv/) for dependency management  

**Install dependencies:**  
```bash
uv sync
```
**Running benchmarks**

Run the pipeline from the project root:

```bash
uv run main.py
```

The pipeline will:

1. Generate all benchmark jobs as defined in the configs (`experiment.yaml`, `dataset_config_map.yaml`, `model_config_map.yaml`).
2. For each job:
   - Load dataset.
   - Apply preprocessing.
   - Configure and run the model.
   - Save predictions, ground truth and configs.
   - Update `benchmark_summary.csv`

Progress is displayed using tqdm.

## ðŸ“– License

MIT License â€“ feel free to use and extend this framework.
